{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clone a private git hub repository"
      ],
      "metadata": {
        "id": "V1LJbJZV8JXB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4c04c44"
      },
      "source": [
        "First, we'll clone the repository using `!git clone`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13b30bbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a88c8542-6aa4-4c35-c49a-71e6f4064c80"
      },
      "source": [
        "!git clone https://github.com/pramendraindian/rag-design-pattern"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rag-design-pattern'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 55 (delta 31), reused 25 (delta 11), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (55/55), 15.29 KiB | 3.82 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d01edad8"
      },
      "source": [
        "After cloning, the repository will be in a directory named `rag-design-pattern`. We need to change our current working directory to this folder to be able to import `main.py` directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e4610ab"
      },
      "source": [
        "import os\n",
        "os.chdir('rag-design-pattern')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b4f9c4f"
      },
      "source": [
        "Now that we are in the repository's directory, you can import `main.py` as a Python module. If `main.py` contains functions or classes, you can then call them."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "ahJjqDCkKPjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d73f9c0f"
      },
      "source": [
        "# To import the main.py file as a module:\n",
        "\n",
        "# Before importing main, set the MODEL_NAME environment variable\n",
        "# with a valid model name, for example, 'google/gemma-2b'\n",
        "import os\n",
        "from google.colab import userdata\n",
        "#Initialize the environment variables\n",
        "os.environ['MODEL_NAME'] = 'google/gemma-2b'\n",
        "os.environ[\"HF_TOKEN\"]=userdata.get('HF_TOKEN')\n",
        "import main\n",
        "\n",
        "# If you wanted to run main.py as a script, you could do:\n",
        "# !python main.py\n",
        "\n",
        "# You can now access functions or variables defined in main.py\n",
        "# For example, if main.py had a function called 'my_function':\n",
        "# main.my_function()\n",
        "#model=main.getModel()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "context = ['Huggin face is provides an interface to connect with llm','Retrieval-Augmented Generation (RAG) is an artificial intelligence framework that enhances large language models (LLMs) by giving them access to external, up-to-date knowledge sources before generating a response']\n",
        "user_query='What is RAG?'\n",
        "\n",
        "# 3. Prepare the input\n",
        "# 4. Construct an enhanced prompt for the LLM\n",
        "# Combine the user query with the retrieved context\n",
        "llm_prompt = f\"\"\"\n",
        "Based on the following information, answer the question:\n",
        "\n",
        "---\n",
        "Context:\n",
        "{\"\\n\".join(context)}\n",
        "---\n",
        "Question: {user_query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(f\"\\nEnhanced LLM Prompt:\\n{llm_prompt}\\n\")\n",
        "\n",
        "# 5. Generate a response using the LLM (model1) with the enhanced prompt\n",
        "inputs=main._getTokenizer(llm_prompt)\n",
        "print('inputs')\n",
        "print(inputs)\n",
        "outputs=main._getModelOutput(inputs)\n",
        "print('outputs')\n",
        "print(outputs[0])\n",
        "decoded_output=main._decodeOutput(outputs)\n",
        "\n",
        "#inputs.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\nLLM Generated Response:\")\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "id": "4LOAg5c1Or8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9bcfae9-c75c-4923-b0ce-906a21284b07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enhanced LLM Prompt:\n",
            "\n",
            "Based on the following information, answer the question:\n",
            "\n",
            "---\n",
            "Context:\n",
            "Huggin face is provides an interface to connect with llm\n",
            "Retrieval-Augmented Generation (RAG) is an artificial intelligence framework that enhances large language models (LLMs) by giving them access to external, up-to-date knowledge sources before generating a response\n",
            "---\n",
            "Question: What is RAG?\n",
            "Answer:\n",
            "\n",
            "inputs\n",
            "{'input_ids': tensor([[     2,    108,  17545,    611,    573,   2412,   2113, 235269,   3448,\n",
            "            573,   2872, 235292,    109,   3976,    108,   2930, 235292,    108,\n",
            "          72342,    778,   3142,    603,   6572,    671,   7222,    577,   9616,\n",
            "            675,  10547, 235262,    108, 232097, 235290,   9531,   7828,  25511,\n",
            "            591, 190757, 235275,    603,    671,  18225,  17273,  15087,    674,\n",
            "          64593,   2910,   5255,   5377,    591,   1650,  14816, 235275,    731,\n",
            "           7385,   1174,   3684,    577,   9774, 235269,    908, 235290,    511,\n",
            "         235290,   1545,   5567,   8269,   1794,  31928,    476,   3590,    108,\n",
            "           3976,    108,   9413, 235292,   2439,    603, 182010, 235336,    108,\n",
            "           1261, 235292]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
            "outputs\n",
            "tensor([     2,    108,  17545,    611,    573,   2412,   2113, 235269,   3448,\n",
            "           573,   2872, 235292,    109,   3976,    108,   2930, 235292,    108,\n",
            "         72342,    778,   3142,    603,   6572,    671,   7222,    577,   9616,\n",
            "           675,  10547, 235262,    108, 232097, 235290,   9531,   7828,  25511,\n",
            "           591, 190757, 235275,    603,    671,  18225,  17273,  15087,    674,\n",
            "         64593,   2910,   5255,   5377,    591,   1650,  14816, 235275,    731,\n",
            "          7385,   1174,   3684,    577,   9774, 235269,    908, 235290,    511,\n",
            "        235290,   1545,   5567,   8269,   1794,  31928,    476,   3590,    108,\n",
            "          3976,    108,   9413, 235292,   2439,    603, 182010, 235336,    108,\n",
            "          1261, 235292, 182010,  12353,    604, 131252, 173038,  25511, 235265,\n",
            "          1165,  22049,   2208,   9774,   5567,   8269,    577,   4771,    573,\n",
            "         10304,  22466,    576,    671,    629,  18622, 235265,    714, 182010,\n",
            "         15087,  15976,  18348,   2149,   7161, 235292,    108, 235290,    714,\n",
            "          1370,   4065,    603,    577,  35126,    573,    629,  18622, 235303,\n",
            "        235256,   7881,   5567,    675,   9774,   5567,   8269],\n",
            "       device='cuda:0')\n",
            "\n",
            "LLM Generated Response:\n",
            "\n",
            "Based on the following information, answer the question:\n",
            "\n",
            "---\n",
            "Context:\n",
            "Huggin face is provides an interface to connect with llm\n",
            "Retrieval-Augmented Generation (RAG) is an artificial intelligence framework that enhances large language models (LLMs) by giving them access to external, up-to-date knowledge sources before generating a response\n",
            "---\n",
            "Question: What is RAG?\n",
            "Answer: RAG stands for Retrieval Augmented Generation. It leverages external knowledge sources to improve the generation capabilities of an LLM. The RAG framework typically involves three steps:\n",
            "- The first step is to augment the LLM's existing knowledge with external knowledge sources\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = ['Huggin face is provides an interface to connect with llm']\n",
        "user_query='What is my name?'\n",
        "llm_prompt = f\"\"\"\n",
        "Based on the following information, answer the question:\n",
        "\n",
        "---\n",
        "Context:\n",
        "{\"\\n\".join(context)}\n",
        "---\n",
        "Question: {user_query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(f\"\\nEnhanced LLM Prompt:\\n{llm_prompt}\\n\")\n",
        "main.generateContent(llm_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dqEvIWQhII-U",
        "outputId": "7f8f3d83-868a-4d8b-f716-f71195a2faf3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enhanced LLM Prompt:\n",
            "\n",
            "Based on the following information, answer the question:\n",
            "\n",
            "---\n",
            "Context:\n",
            "Huggin face is provides an interface to connect with llm\n",
            "---\n",
            "Question: What is my name?\n",
            "Answer:\n",
            "\n",
            "User Prompt \n",
            "Based on the following information, answer the question:\n",
            "\n",
            "---\n",
            "Context:\n",
            "Huggin face is provides an interface to connect with llm\n",
            "---\n",
            "Question: What is my name?\n",
            "Answer:\n",
            "input token {'input_ids': tensor([[     2,    108,  17545,    611,    573,   2412,   2113, 235269,   3448,\n",
            "            573,   2872, 235292,    109,   3976,    108,   2930, 235292,    108,\n",
            "          72342,    778,   3142,    603,   6572,    671,   7222,    577,   9616,\n",
            "            675,  10547, 235262,    108,   3976,    108,   9413, 235292,   2439,\n",
            "            603,    970,   1503, 235336,    108,   1261, 235292]],\n",
            "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
            "       device='cuda:0')}\n",
            "raw outputs tensor([[     2,    108,  17545,    611,    573,   2412,   2113, 235269,   3448,\n",
            "            573,   2872, 235292,    109,   3976,    108,   2930, 235292,    108,\n",
            "          72342,    778,   3142,    603,   6572,    671,   7222,    577,   9616,\n",
            "            675,  10547, 235262,    108,   3976,    108,   9413, 235292,   2439,\n",
            "            603,    970,   1503, 235336,    108,   1261, 235292,    108,   6750,\n",
            "           2024,    590, 235349, 235262,  25660,    778,  20930, 239880, 235265,\n",
            "           2551,    692,  10200,   1212,    603,  50280, 235284, 235304, 235265,\n",
            "            108,   1261, 235292,    108,  18622, 235284, 235304,    603,    476,\n",
            "          10629,    577,  10629,   6011,    604,  17183,   1865,  17044, 235265,\n",
            "           1165,    603, 209099,    578,   2011, 235290,  45163,  15613,    603,\n",
            "           1671, 235265,    108]], device='cuda:0')\n",
            "decoded outputs \n",
            "Based on the following information, answer the question:\n",
            "\n",
            "---\n",
            "Context:\n",
            "Huggin face is provides an interface to connect with llm\n",
            "---\n",
            "Question: What is my name?\n",
            "Answer:\n",
            "Hey!! Iâ€™m Huggin FaceðŸ¤—. Can you explain what is LM23.\n",
            "Answer:\n",
            "LM23 is a sequence to sequence network for translation between languages. It is bidirectional and self-attention mechanism is used.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBased on the following information, answer the question:\\n\\n---\\nContext:\\nHuggin face is provides an interface to connect with llm\\n---\\nQuestion: What is my name?\\nAnswer:\\nHey!! Iâ€™m Huggin FaceðŸ¤—. Can you explain what is LM23.\\nAnswer:\\nLM23 is a sequence to sequence network for translation between languages. It is bidirectional and self-attention mechanism is used.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}