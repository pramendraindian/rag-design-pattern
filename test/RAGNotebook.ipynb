{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clone a private git hub repository"
      ],
      "metadata": {
        "id": "V1LJbJZV8JXB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4c04c44"
      },
      "source": [
        "First, we'll clone the repository using `!git clone`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13b30bbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3b42386-9718-453d-bdca-663fc310282a"
      },
      "source": [
        "!git clone https://github.com/pramendraindian/rag-design-pattern"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rag-design-pattern'...\n",
            "remote: Enumerating objects: 59, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 59 (delta 32), reused 28 (delta 11), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (59/59), 18.68 KiB | 6.23 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d01edad8"
      },
      "source": [
        "After cloning, the repository will be in a directory named `rag-design-pattern`. We need to change our current working directory to this folder to be able to import `main.py` directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e4610ab"
      },
      "source": [
        "import os\n",
        "os.chdir('rag-design-pattern')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b4f9c4f"
      },
      "source": [
        "Now that we are in the repository's directory, you can import `main.py` as a Python module. If `main.py` contains functions or classes, you can then call them."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "ahJjqDCkKPjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d73f9c0f"
      },
      "source": [
        "# To import the main.py file as a module:\n",
        "\n",
        "# Before importing main, set the MODEL_NAME environment variable\n",
        "# with a valid model name, for example, 'google/gemma-2b'\n",
        "import os\n",
        "from google.colab import userdata\n",
        "#Initialize the environment variables\n",
        "os.environ['MODEL_NAME'] = 'google/gemma-2b'\n",
        "os.environ[\"HF_TOKEN\"]=userdata.get('HF_TOKEN')\n",
        "import main\n",
        "\n",
        "# If you wanted to run main.py as a script, you could do:\n",
        "# !python main.py\n",
        "\n",
        "# You can now access functions or variables defined in main.py\n",
        "# For example, if main.py had a function called 'my_function':\n",
        "# main.my_function()\n",
        "#model=main.getModel()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "context = ['Huggin face is provides an interface to connect with llm',\n",
        "           'Retrieval-Augmented Generation (RAG) is an artificial intelligence framework that enhances large language models (LLMs) by giving them access to external, up-to-date knowledge sources before generating a response'\n",
        "          ]\n",
        "user_query='What is RAG?'\n",
        "\n",
        "# 3. Prepare the input\n",
        "# 4. Construct an enhanced prompt for the LLM\n",
        "# Combine the user query with the retrieved context\n",
        "llm_prompt = f\"\"\"\n",
        "Based on the following information, answer the question:\n",
        "\n",
        "---\n",
        "Context:\n",
        "{\"\\n\".join(context)}\n",
        "---\n",
        "Question: {user_query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(f\"\\nEnhanced LLM Prompt:\\n{llm_prompt}\\n\")\n",
        "\n",
        "# 5. Generate a response using the LLM (model1) with the enhanced prompt\n",
        "inputs=main._getTokenizer(llm_prompt)\n",
        "print('inputs')\n",
        "print(inputs)\n",
        "outputs=main._getModelOutput(inputs)\n",
        "print('outputs')\n",
        "print(outputs[0])\n",
        "decoded_output=main._decodeOutput(outputs)\n",
        "\n",
        "#inputs.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\nLLM Generated Response:\")\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "id": "4LOAg5c1Or8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63824096-064d-4c42-941f-a7a1499efc7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enhanced LLM Prompt:\n",
            "\n",
            "Based on the following information, answer the question:\n",
            "\n",
            "---\n",
            "Context:\n",
            "Huggin face is provides an interface to connect with llm\n",
            "Retrieval-Augmented Generation (RAG) is an artificial intelligence framework that enhances large language models (LLMs) by giving them access to external, up-to-date knowledge sources before generating a response\n",
            "---\n",
            "Question: What is RAG?\n",
            "Answer:\n",
            "\n",
            "inputs\n",
            "{'input_ids': tensor([[     2,    108,  17545,    611,    573,   2412,   2113, 235269,   3448,\n",
            "            573,   2872, 235292,    109,   3976,    108,   2930, 235292,    108,\n",
            "          72342,    778,   3142,    603,   6572,    671,   7222,    577,   9616,\n",
            "            675,  10547, 235262,    108, 232097, 235290,   9531,   7828,  25511,\n",
            "            591, 190757, 235275,    603,    671,  18225,  17273,  15087,    674,\n",
            "          64593,   2910,   5255,   5377,    591,   1650,  14816, 235275,    731,\n",
            "           7385,   1174,   3684,    577,   9774, 235269,    908, 235290,    511,\n",
            "         235290,   1545,   5567,   8269,   1794,  31928,    476,   3590,    108,\n",
            "           3976,    108,   9413, 235292,   2439,    603, 182010, 235336,    108,\n",
            "           1261, 235292]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
            "outputs\n",
            "tensor([     2,    108,  17545,    611,    573,   2412,   2113, 235269,   3448,\n",
            "           573,   2872, 235292,    109,   3976,    108,   2930, 235292,    108,\n",
            "         72342,    778,   3142,    603,   6572,    671,   7222,    577,   9616,\n",
            "           675,  10547, 235262,    108, 232097, 235290,   9531,   7828,  25511,\n",
            "           591, 190757, 235275,    603,    671,  18225,  17273,  15087,    674,\n",
            "         64593,   2910,   5255,   5377,    591,   1650,  14816, 235275,    731,\n",
            "          7385,   1174,   3684,    577,   9774, 235269,    908, 235290,    511,\n",
            "        235290,   1545,   5567,   8269,   1794,  31928,    476,   3590,    108,\n",
            "          3976,    108,   9413, 235292,   2439,    603, 182010, 235336,    108,\n",
            "          1261, 235292, 182010,    603,    476,   2370,    576,  55589,   2910,\n",
            "          5255,   5377,    591,   1650,  14816, 235275,    731,   7385,   1174,\n",
            "          3684,    577,   9774,   5567,   8269,   1794,  31928,    476,   3590,\n",
            "           108,      1], device='cuda:0')\n",
            "\n",
            "LLM Generated Response:\n",
            "\n",
            "Based on the following information, answer the question:\n",
            "\n",
            "---\n",
            "Context:\n",
            "Huggin face is provides an interface to connect with llm\n",
            "Retrieval-Augmented Generation (RAG) is an artificial intelligence framework that enhances large language models (LLMs) by giving them access to external, up-to-date knowledge sources before generating a response\n",
            "---\n",
            "Question: What is RAG?\n",
            "Answer: RAG is a method of enhancing large language models (LLMs) by giving them access to external knowledge sources before generating a response\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = ['Huggin face is provides an interface to connect with llm']\n",
        "user_query='What is my name?'\n",
        "llm_prompt = f\"\"\"\n",
        "Based on the following information, answer the question:\n",
        "\n",
        "---\n",
        "Context:\n",
        "{\"\\n\".join(context)}\n",
        "---\n",
        "Question: {user_query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(f\"\\nEnhanced LLM Prompt:\\n{llm_prompt}\\n\")\n",
        "answer=main.generateContent(llm_prompt)\n",
        "print(f\"\\n ###################################################\\n\")\n",
        "print(f\"\\n Final Answer :\\n\",answer)\n",
        "print(f\"\\n ###################################################\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqEvIWQhII-U",
        "outputId": "d4a245b9-3813-4733-a017-2b7f3cce615d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enhanced LLM Prompt:\n",
            "\n",
            "Based on the following information, answer the question:\n",
            "\n",
            "---\n",
            "Context:\n",
            "Huggin face is provides an interface to connect with llm\n",
            "---\n",
            "Question: What is my name?\n",
            "Answer:\n",
            "\n",
            "User Prompt \n",
            "Based on the following information, answer the question:\n",
            "\n",
            "---\n",
            "Context:\n",
            "Huggin face is provides an interface to connect with llm\n",
            "---\n",
            "Question: What is my name?\n",
            "Answer:\n",
            "input token {'input_ids': tensor([[     2,    108,  17545,    611,    573,   2412,   2113, 235269,   3448,\n",
            "            573,   2872, 235292,    109,   3976,    108,   2930, 235292,    108,\n",
            "          72342,    778,   3142,    603,   6572,    671,   7222,    577,   9616,\n",
            "            675,  10547, 235262,    108,   3976,    108,   9413, 235292,   2439,\n",
            "            603,    970,   1503, 235336,    108,   1261, 235292]],\n",
            "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
            "       device='cuda:0')}\n",
            "raw outputs tensor([[     2,    108,  17545,    611,    573,   2412,   2113, 235269,   3448,\n",
            "            573,   2872, 235292,    109,   3976,    108,   2930, 235292,    108,\n",
            "          72342,    778,   3142,    603,   6572,    671,   7222,    577,   9616,\n",
            "            675,  10547, 235262,    108,   3976,    108,   9413, 235292,   2439,\n",
            "            603,    970,   1503, 235336,    108,   1261, 235292,    108,   1718,\n",
            "           6572,    108, 235248,    110,   9413, 235248, 235274, 235292,    590,\n",
            "           1938,    577,   1230,    573,   6996,    576,    591, 235285,   1938,\n",
            "            577,   1230,   1968,    108,    886,   4158,   5255,  10310,   1688,\n",
            "            665,    603,   1671,    685,   1688,    108,   8036, 235292,    590,\n",
            "           1938,    577,   1230,    573,   6996,    576,    591, 235285,   1938,\n",
            "            577,   1230,   1968]], device='cuda:0')\n",
            "decoded outputs \n",
            "Based on the following information, answer the question:\n",
            "\n",
            "---\n",
            "Context:\n",
            "Huggin face is provides an interface to connect with llm\n",
            "---\n",
            "Question: What is my name?\n",
            "Answer:\n",
            "It provides\n",
            " \n",
            "\n",
            "\n",
            "Question 1: I want to know the meaning of (I want to know )\n",
            "In natural language processing , it is used as ,\n",
            "Example: I want to know the meaning of (I want to know )\n",
            "\n",
            " ###################################################\n",
            "\n",
            "\n",
            " Final Answer :\n",
            " \n",
            "Based on the following information, answer the question:\n",
            "\n",
            "---\n",
            "Context:\n",
            "Huggin face is provides an interface to connect with llm\n",
            "---\n",
            "Question: What is my name?\n",
            "Answer:\n",
            "It provides\n",
            " \n",
            "\n",
            "\n",
            "Question 1: I want to know the meaning of (I want to know )\n",
            "In natural language processing , it is used as ,\n",
            "Example: I want to know the meaning of (I want to know )\n",
            "\n",
            " ###################################################\n",
            "\n"
          ]
        }
      ]
    }
  ]
}